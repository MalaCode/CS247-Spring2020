{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coordinates',\n",
       " 'created_at',\n",
       " 'hashtags',\n",
       " 'media',\n",
       " 'urls',\n",
       " 'favorite_count',\n",
       " 'id',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_user_id',\n",
       " 'lang',\n",
       " 'place',\n",
       " 'possibly_sensitive',\n",
       " 'retweet_count',\n",
       " 'reweet_id',\n",
       " 'retweet_screen_name',\n",
       " 'source',\n",
       " 'text',\n",
       " 'tweet_url',\n",
       " 'user_created_at',\n",
       " 'user_screen_name',\n",
       " 'user_default_profile_image',\n",
       " 'user_description',\n",
       " 'user_favourites_count',\n",
       " 'user_followers_count',\n",
       " 'user_friends_count',\n",
       " 'user_listed_count',\n",
       " 'user_location',\n",
       " 'user_name',\n",
       " 'user_screen_name.1',\n",
       " 'user_statuses_count',\n",
       " 'user_time_zone',\n",
       " 'user_urls',\n",
       " 'user_verified']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "data = pd.read_csv(\"sample_data.csv\")\n",
    "list(data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27666\n",
      "We will not meet our monthly fundraising goal for this month and that is ok. \n",
      "\n",
      "We will continue fighting. Come join us when you are ready and able. We'll leave a spot for you.\n",
      "\n",
      "Very grateful for the people who have contributed and the donors who have offered matching funds. https://t.co/qmDQIX05Ms\n"
     ]
    }
   ],
   "source": [
    "df = DataFrame(data, columns = data.columns.values)\n",
    "df = df.loc[df[\"lang\"] == \"en\"]\n",
    "tweet_text = df[\"text\"]\n",
    "tweet_text['index'] = tweet_text.index\n",
    "tweets = tweet_text\n",
    "print(len(tweets))\n",
    "print(tweets[2410])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/fayland/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Functions to perform lemmatize and stem preprocessing steps on the data set. (from  https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the stemming function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  ['meet', 'monthly', 'fundraising', 'goal', 'month', '\\n\\nWe', 'continue', 'fighting.', 'Come', 'join', 'ready', 'able.', \"We'll\", 'leave', 'spot', 'you.\\n\\nVery', 'grateful', 'people', 'contributed', 'donors', 'offered', 'matching', 'funds.', 'https://t.co/qmDQIX05Ms']\n",
      "Processed: ['meet', 'month', 'fundrais', 'goal', 'month', 'continu', 'fight', 'come', 'join', 'readi', 'abl', 'leav', 'spot', 'grate', 'peopl', 'contribut', 'donor', 'offer', 'match', 'fund', 'https', 'qmdqix']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for word in tweets[2410].split(' '):\n",
    "    if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 3:\n",
    "        words.append(word)\n",
    "print(\"original: \",words)\n",
    "sample_process = preprocess(tweets[2410])\n",
    "print(\"Processed:\", sample_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     [wionew, gravita, china, biggest, strain, viru...\n",
       "2     [dailycal, rick, scott, want, probe, china, co...\n",
       "3     [farouqsajoh, today, mark, lockdown, abuja, re...\n",
       "4     [mailonlin, wuhan, doctor, alert, medic, sprea...\n",
       "5     [christufton, dear, jamaican, social, distanc,...\n",
       "6     [kt_so_it_go, tomorrow, realiti, host, turn, m...\n",
       "7     [spectatorindex, coronavirus, death, itali, sp...\n",
       "8           [jrocismajor_, florida, liter, nigga, care]\n",
       "10    [gayconservativ, busi, bing, compani, busi, ma...\n",
       "11    [adityarajkaul, indian, prime, minist, narendr...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets = tweets.fillna('').astype(str).map(preprocess)\n",
    "processed_tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 31149\n",
      "0 biggest\n",
      "1 china\n",
      "2 equip\n",
      "3 gravita\n",
      "4 handl\n",
      "5 keep\n",
      "6 sampl\n",
      "7 strain\n",
      "8 virus\n",
      "9 wionew\n",
      "10 cover\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_tweets)\n",
    "print(\"dictionary size:\", len(dictionary))\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out tokens that appear in\n",
    "less than 15 documents (absolute number) or\n",
    "more than 0.8 documents (fraction of total corpus size, not absolute number).\n",
    "after the above two steps, keep only the first 100000 most frequent tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: 2316\n",
      "0 biggest\n",
      "1 china\n",
      "2 equip\n",
      "3 handl\n",
      "4 keep\n",
      "5 sampl\n",
      "6 virus\n",
      "7 cover\n",
      "8 dailycal\n",
      "9 https\n",
      "10 rick\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "print(\"dictionary size:\", len(dictionary))\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 1),\n",
       " (81, 1),\n",
       " (95, 1),\n",
       " (151, 1),\n",
       " (198, 1),\n",
       " (257, 1),\n",
       " (258, 1),\n",
       " (259, 1),\n",
       " (847, 1),\n",
       " (1067, 1),\n",
       " (1940, 1)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(tweet) for tweet in processed_tweets]\n",
    "bow_corpus[2410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 2316\n",
      "Number of documents: 27666\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 5\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -5.8246.\n",
      "[([(0.08420193, 'https'),\n",
      "   (0.045797788, 'coronavirus'),\n",
      "   (0.033386055, 'covid'),\n",
      "   (0.027639877, 'trump'),\n",
      "   (0.018805657, 'pandem'),\n",
      "   (0.016973855, 'china'),\n",
      "   (0.016536236, 'say'),\n",
      "   (0.016375512, 'peopl'),\n",
      "   (0.011346821, 'time'),\n",
      "   (0.011289188, 'like'),\n",
      "   (0.009991487, 'know'),\n",
      "   (0.009387468, 'presid'),\n",
      "   (0.008708884, 'go'),\n",
      "   (0.008301215, 'virus'),\n",
      "   (0.0074680247, 'work'),\n",
      "   (0.0073828166, 'american'),\n",
      "   (0.0065728123, 'need'),\n",
      "   (0.0063553127, 'respons'),\n",
      "   (0.006340456, 'help'),\n",
      "   (0.0062009273, 'think')],\n",
      "  -3.3907247196380776),\n",
      " ([(0.026925016, 'covid'),\n",
      "   (0.026822621, 'test'),\n",
      "   (0.025913974, 'case'),\n",
      "   (0.025300793, 'coronavirus'),\n",
      "   (0.022030132, 'lockdown'),\n",
      "   (0.019439049, 'death'),\n",
      "   (0.015253407, 'health'),\n",
      "   (0.014799189, 'report'),\n",
      "   (0.014494035, 'posit'),\n",
      "   (0.013674314, 'state'),\n",
      "   (0.013092979, 'hospit'),\n",
      "   (0.011997539, 'come'),\n",
      "   (0.009235028, 'break'),\n",
      "   (0.008878068, 'million'),\n",
      "   (0.008697615, 'patient'),\n",
      "   (0.008620247, 'number'),\n",
      "   (0.0083250515, 'confirm'),\n",
      "   (0.008191948, 'watch'),\n",
      "   (0.008029197, 'updat'),\n",
      "   (0.0078100646, 'public')],\n",
      "  -4.208137105729596),\n",
      " ([(0.04402689, 'stay'),\n",
      "   (0.03973908, 'home'),\n",
      "   (0.026231183, 'corona'),\n",
      "   (0.024114966, 'social'),\n",
      "   (0.01978103, 'distanc'),\n",
      "   (0.018168475, 'year'),\n",
      "   (0.01682119, 'tell'),\n",
      "   (0.015529724, 'peopl'),\n",
      "   (0.0122640245, 'order'),\n",
      "   (0.01206857, 'day'),\n",
      "   (0.009712304, 'april'),\n",
      "   (0.009633984, 'hold'),\n",
      "   (0.009521262, 'ask'),\n",
      "   (0.008816567, 'start'),\n",
      "   (0.008210422, 'free'),\n",
      "   (0.008111595, 'issu'),\n",
      "   (0.007997965, 'question'),\n",
      "   (0.007791242, 'ralli'),\n",
      "   (0.007744892, 'live'),\n",
      "   (0.007657469, 'shit')],\n",
      "  -5.720816367015522),\n",
      " ([(0.04149279, 'impeach'),\n",
      "   (0.022598406, 'trump'),\n",
      "   (0.02114316, 'mcconnel'),\n",
      "   (0.017318137, 'realdonaldtrump'),\n",
      "   (0.015247201, 'govern'),\n",
      "   (0.014131981, 'attent'),\n",
      "   (0.014092015, 'distract'),\n",
      "   (0.014081186, 'spread'),\n",
      "   (0.012644228, 'slow'),\n",
      "   (0.012626286, 'day'),\n",
      "   (0.012434087, 'gtconway'),\n",
      "   (0.012313553, 'senat'),\n",
      "   (0.012186769, 'wuhan'),\n",
      "   (0.0121479975, 'mitch'),\n",
      "   (0.01210435, 'virus'),\n",
      "   (0.011797179, 'democrat'),\n",
      "   (0.011716638, 'covidー'),\n",
      "   (0.011079503, 'today'),\n",
      "   (0.010486208, 'focus'),\n",
      "   (0.010152846, 'write')],\n",
      "  -7.062576294752937),\n",
      " ([(0.01927661, 'protect'),\n",
      "   (0.019061342, 'covid'),\n",
      "   (0.01414451, 'person'),\n",
      "   (0.012667016, 'nation'),\n",
      "   (0.012413545, 'donat'),\n",
      "   (0.010462419, 'plasma'),\n",
      "   (0.010162274, 'direct'),\n",
      "   (0.009847093, 'project'),\n",
      "   (0.009424276, 'presid'),\n",
      "   (0.009350475, 'american'),\n",
      "   (0.009129622, 'unit'),\n",
      "   (0.008583949, 'plan'),\n",
      "   (0.008126151, 'help'),\n",
      "   (0.0079822615, 'blood'),\n",
      "   (0.007915887, 'state'),\n",
      "   (0.0077919085, 'today'),\n",
      "   (0.007783377, 'busi'),\n",
      "   (0.007736048, 'administr'),\n",
      "   (0.0074036657, 'smart'),\n",
      "   (0.007253569, 'desanti')],\n",
      "  -8.740929553867051)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(bow_corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
